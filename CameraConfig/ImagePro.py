import os

import cv2
import numpy as np
from functools import lru_cache

# å…¨å±€å˜é‡ï¼Œç”¨äºå­˜å‚¨é¢„åŠ è½½çš„æ¨¡æ¿
templateNeedle = None
templateNeedle_size = (0, 0)
templateDevice = None
templateDevice_size = (0, 0)
templateLight = None
templateLight_size = (0, 0)


# ç¼“å­˜æ¨¡æ¿åŠ è½½ç»“æœ
def load_templates():
    global templateNeedle, templateNeedle_size, templateDevice, templateDevice_size, templateLight, templateLight_size
    # å®šä¹‰æ¨¡æ¿è·¯å¾„å’Œå¯¹åº”çš„å…¨å±€å˜é‡
    templates = {
        'templateNeedle.png': ('templateNeedle', 'templateNeedle_size'),
        'templatepad.png': ('templateDevice', 'templateDevice_size'),
        'templateLight.png': ('templateLight', 'templateLight_size')
    }

    # æ£€æŸ¥æ¯ä¸ªæ¨¡æ¿æ–‡ä»¶
    for path, (template_var, size_var) in templates.items():
        if os.path.exists(path):
            current_mtime = os.path.getmtime(path)

            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½
            cache_key = f"{template_var}_mtime"
            if not hasattr(load_templates, cache_key) or getattr(load_templates, cache_key) != current_mtime:
                # æ–‡ä»¶å‘ç”Ÿå˜åŒ–æˆ–é¦–æ¬¡åŠ è½½ï¼Œé‡æ–°è¯»å–
                template_img = cv2.imread(path, cv2.IMREAD_COLOR)
                if template_img is not None:
                    globals()[template_var] = template_img
                    globals()[size_var] = template_img.shape[:2]
                    setattr(load_templates, cache_key, current_mtime)
                    print(f"Loaded template: {path}")
                else:
                    print(f"Failed to load template image from {path}")
            # else: æ–‡ä»¶æœªå˜åŒ–ï¼Œä½¿ç”¨ç¼“å­˜ä¸­çš„æ¨¡æ¿
        else:
            print(f"Template file not found: {path}")


def is_nearby_vectorized(centers_np, x, y, min_distance):
    """
    ä½¿ç”¨å‘é‡åŒ–æ–¹å¼åˆ¤æ–­ä¸€ä¸ªç‚¹æ˜¯å¦é è¿‘å·²å­˜åœ¨çš„ç‚¹ã€‚
    ä¼˜åŒ–: ä½¿ç”¨å¹³æ–¹è·ç¦»é¿å…sqrtè®¡ç®—
    """
    if centers_np.size == 0:
        return False
    squared_distances = (centers_np[:, 0] - x) ** 2 + (centers_np[:, 1] - y) ** 2
    return np.any(squared_distances < min_distance ** 2)

import cv2
import numpy as np


def template(video, x_dia=0, y_dia=0, equipment=0, sharpen_params=None):
    """å¿«é€Ÿç¨³å®šçš„æ¨¡æ¿åŒ¹é…ç‰ˆæœ¬ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    global templateNeedle, templateLight

    # ================================= æ­¥éª¤1ï¼šå¼ºåˆ¶è½¬æ¢ä¸ºç°åº¦å›¾åƒ =================================
    if len(video.shape) == 3:
        video_gray = cv2.cvtColor(video, cv2.COLOR_BGR2GRAY)
    else:
        video_gray = video

    # ================================= æ­¥éª¤2ï¼šè·å–æ¨¡æ¿å¹¶ç°åº¦åŒ– =================================
    template_img = templateLight if equipment else templateNeedle
    if template_img is None:
        return None, None, 0, 0

    if len(template_img.shape) == 3:
        template_gray = cv2.cvtColor(template_img, cv2.COLOR_BGR2GRAY)
    else:
        template_gray = template_img

    # ================================= æ­¥éª¤3ï¼šè½»é‡é¢„å¤„ç†ï¼ˆå‡å°‘ç‰¹å¾ç ´åï¼‰=================================
    # ğŸ”´ ä¼˜åŒ–ï¼šå‡å°‘é¢„å¤„ç†å¼ºåº¦ï¼Œé¿å…ç ´ååŸå§‹ç‰¹å¾
    # ä»…åœ¨éœ€è¦æ—¶è¿›è¡Œè½»å¾®å¤„ç†
    video_processed = cv2.GaussianBlur(video_gray, (3, 3), 0)  # è½»å¾®å»å™ª
    template_processed = cv2.GaussianBlur(template_gray, (3, 3), 0)

    # ================================= æ­¥éª¤4ï¼šå¤šå°ºåº¦æ¨¡æ¿åŒ¹é… =================================
    # ğŸ”´ ä¼˜åŒ–ï¼šæ·»åŠ å¤šå°ºåº¦åŒ¹é…ï¼Œæé«˜é²æ£’æ€§
    scales = [0.9, 1.0, 1.1]  # æ¢é’ˆå¯èƒ½å› è·ç¦»ç•¥æœ‰ç¼©æ”¾
    best_val = -1
    best_loc = None
    best_scale = 1.0
    best_template = template_processed
    
    for scale in scales:
        # ç¼©æ”¾æ¨¡æ¿
        if scale != 1.0:
            h, w = template_processed.shape
            new_h, new_w = int(h * scale), int(w * scale)
            if new_h < 10 or new_w < 10:
                continue
            scaled_template = cv2.resize(template_processed, (new_w, new_h))
        else:
            scaled_template = template_processed
        
        # ğŸ”´ ä¼˜åŒ–ï¼šåªä½¿ç”¨æœ€å¯é çš„ TM_CCOEFF_NORMED æ–¹æ³•
        res = cv2.matchTemplate(video_processed, scaled_template, cv2.TM_CCOEFF_NORMED)
        _, max_val, _, max_loc = cv2.minMaxLoc(res)
        
        if max_val > best_val:
            best_val = max_val
            best_loc = max_loc
            best_scale = scale
            best_template = scaled_template

    # ================================= æ­¥éª¤5ï¼šå±€éƒ¨ç²¾ç»†åŒ–æœç´¢ =================================
    # ğŸ”´ ä¼˜åŒ–ï¼šé™ä½é˜ˆå€¼ä» 0.9 åˆ° 0.65ï¼Œæ¥å—æ›´å¤šæœ‰æ•ˆåŒ¹é…
    if best_val > 0.65:
        x, y = best_loc
        h, w = best_template.shape

        # å±€éƒ¨ç²¾ç»†åŒ–æœç´¢ï¼ˆä»…åœ¨å®‰å…¨èŒƒå›´å†…ï¼‰
        if x > 5 and y > 5 and x + w < video_processed.shape[1] - 5 and y + h < video_processed.shape[0] - 5:
            roi = video_processed[y - 5:y + h + 5, x - 5:x + w + 5]
            refined_res = cv2.matchTemplate(roi, best_template, cv2.TM_CCOEFF_NORMED)
            _, refined_val, _, refined_loc = cv2.minMaxLoc(refined_res)

            if refined_val > best_val:
                x = x - 5 + refined_loc[0]
                y = y - 5 + refined_loc[1]
                best_val = refined_val

        # è®¡ç®—çº¢ç‚¹ä¸­å¿ƒå¹¶ç»˜åˆ¶ï¼ˆä½¿ç”¨åŸå§‹æ¨¡æ¿å°ºå¯¸ï¼‰
        orig_h, orig_w = template_gray.shape
        red_dot_x = int(x + (w / best_scale) // 2 + x_dia)
        red_dot_y = int(y + (h / best_scale) // 2 + y_dia)
        
        # ğŸ”´ ä¼˜åŒ–ï¼šæ ¹æ®åŒ¹é…å¾—åˆ†è°ƒæ•´çº¢ç‚¹é¢œè‰²ï¼ˆç½®ä¿¡åº¦å¯è§†åŒ–ï¼‰
        if best_val > 0.85:
            color = (0, 255, 0)  # ç»¿è‰² - é«˜ç½®ä¿¡åº¦
        elif best_val > 0.75:
            color = (0, 165, 255)  # æ©™è‰² - ä¸­ç­‰ç½®ä¿¡åº¦
        else:
            color = (0, 0, 255)  # çº¢è‰² - ä½ç½®ä¿¡åº¦
        
        cv2.circle(video, (red_dot_x, red_dot_y), 5, color, -1)
        
        # ğŸ”´ ä¼˜åŒ–ï¼šå§‹ç»ˆæ˜¾ç¤ºåŒ¹é…æ¡†å’Œå¾—åˆ†ï¼ˆå¸®åŠ©è°ƒè¯•ï¼‰
        # cv2.rectangle(video, (x, y), (x + w, y + h), (0, 255, 0), 1)  # ç»¿è‰²åŒ¹é…æ¡†
        # cv2.putText(video, f"{best_val:.2f}", (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 0), 1)

        return red_dot_x, red_dot_y, template_img.shape[0], template_img.shape[1]
    else:
        # ğŸ”´ ä¼˜åŒ–ï¼šæ˜¾ç¤ºå®é™…åŒ¹é…å¾—åˆ†
        cv2.putText(video, f"No Match (Score={best_val:.2f}, Need>0.65)", (10, 30), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
        return None, None, 0, 0


def preprocess_images(video_gray, template_gray, sharpen_params=None):
    """ç»¼åˆå›¾åƒé¢„å¤„ç†å‡½æ•°"""
    # é»˜è®¤é”åŒ–å‚æ•°
    if sharpen_params is None:
        sharpen_params = {'kernel_size': 3, 'sigma': 1.0, 'amount': 1.5, 'threshold': 10}

    # 1. ç›´æ–¹å›¾å‡è¡¡åŒ–ï¼ˆå¢å¼ºå¯¹æ¯”åº¦ï¼‰
    video_gray = cv2.equalizeHist(video_gray)
    template_gray = cv2.equalizeHist(template_gray)

    # 2. é«˜æ–¯æ¨¡ç³Šå»å™ªï¼ˆè½»å¾®ï¼‰
    video_gray = cv2.GaussianBlur(video_gray, (3, 3), 0)
    template_gray = cv2.GaussianBlur(template_gray, (3, 3), 0)

    # 3. é”åŒ–å¤„ç†
    video_gray = sharpen_image(video_gray, **sharpen_params)
    template_gray = sharpen_image(template_gray, **sharpen_params)

    # 4. è¾¹ç¼˜å¢å¼ºï¼ˆå¯é€‰ï¼Œå¯¹äºè¾¹ç¼˜æ˜æ˜¾çš„æ¢é’ˆå¾ˆæœ‰æ•ˆï¼‰
    # video_gray = enhance_edges(video_gray)
    # template_gray = enhance_edges(template_gray)

    return video_gray, template_gray


def sharpen_image(image, kernel_size=3, sigma=1.0, amount=1.5, threshold=5):
    """USMé”åŒ–ç®—æ³•"""
    # é«˜æ–¯æ¨¡ç³Š
    blurred = cv2.GaussianBlur(image, (kernel_size, kernel_size), sigma)

    # éé”åŒ–æ©è”½
    sharpened = cv2.addWeighted(image, 1.0 + amount, blurred, -amount, 0)

    # é˜ˆå€¼å¤„ç†ï¼Œé¿å…è¿‡åº¦é”åŒ–å¹³æ»‘åŒºåŸŸ
    if threshold > 0:
        low_contrast_mask = np.abs(image - blurred) < threshold
        sharpened[low_contrast_mask] = image[low_contrast_mask]

    return sharpened


def enhance_edges(image, low_threshold=50, high_threshold=150):
    """è¾¹ç¼˜å¢å¼º"""
    # Cannyè¾¹ç¼˜æ£€æµ‹
    edges = cv2.Canny(image, low_threshold, high_threshold)

    # å°†è¾¹ç¼˜å åŠ åˆ°åŸå›¾
    enhanced = cv2.addWeighted(image, 0.8, edges, 0.2, 0)

    return enhanced


# å¯é€‰ï¼šè‡ªé€‚åº”é¢„å¤„ç†ï¼ˆæ ¹æ®å›¾åƒç‰¹æ€§è‡ªåŠ¨è°ƒæ•´ï¼‰
def adaptive_preprocess(image):
    """è‡ªé€‚åº”é¢„å¤„ç†"""
    # è®¡ç®—å›¾åƒå¯¹æ¯”åº¦
    mean, std = cv2.meanStdDev(image)
    contrast = std[0][0]

    if contrast < 25:  # ä½å¯¹æ¯”åº¦å›¾åƒ
        image = cv2.equalizeHist(image)
        image = sharpen_image(image, amount=2.0)
    elif contrast > 60:  # é«˜å¯¹æ¯”åº¦å›¾åƒ
        image = cv2.GaussianBlur(image, (3, 3), 0)
    else:  # ä¸­ç­‰å¯¹æ¯”åº¦
        image = sharpen_image(image, amount=1.5)

    return image


def match_device_templates(video):
    global templateDevice, templateDevice_size

    if templateDevice is None:
        print("æ¨¡æ¿æœªåŠ è½½ï¼Œæ— æ³•è¿›è¡ŒåŒ¹é…")
        return []

    try:
        with open('Paddia.txt', 'r', encoding='utf-8') as f:
            xdia, ydia = map(int, f.read().strip().split(','))
    except:
        xdia, ydia = 0, 0

    # ç°åº¦è½¬æ¢
    video_gray = cv2.cvtColor(video, cv2.COLOR_BGR2GRAY) if len(video.shape) == 3 else video
    template_gray = cv2.cvtColor(templateDevice, cv2.COLOR_BGR2GRAY) if len(
        templateDevice.shape) == 3 else templateDevice

    original_template_h, original_template_w = template_gray.shape

    # å¤šå°ºåº¦åŒ¹é…
    scales = [0.8, 0.9, 1.0, 1.1, 1.2]
    all_matches = []

    for scale in scales:
        # ç¼©æ”¾æ¨¡æ¿
        if scale != 1.0:
            new_w = int(original_template_w * scale)
            new_h = int(original_template_h * scale)
            if new_w < 10 or new_h < 10: continue
            template_resized = cv2.resize(template_gray, (new_w, new_h))
        else:
            template_resized = template_gray

        # æ‰§è¡ŒåŒ¹é…
        res = cv2.matchTemplate(video_gray, template_resized, cv2.TM_CCOEFF_NORMED)

        # æ›´ä¸¥æ ¼çš„é˜ˆå€¼è®¾ç½®
        threshold = max(0.85, 0.8)  # ä½¿ç”¨å›ºå®šé«˜é˜ˆå€¼æˆ–æ›´æ™ºèƒ½çš„åŠ¨æ€é˜ˆå€¼
        locs = np.where(res >= threshold)

        # åªä¿ç•™æ¯ä¸ªå±€éƒ¨åŒºåŸŸçš„æœ€ä½³åŒ¹é…
        temp_matches = []
        for pt in zip(*locs[::-1]):
            temp_matches.append({
                'pt': pt,
                'size': (template_resized.shape[1], template_resized.shape[0]),
                'score': res[pt[1], pt[0]],
                'scale': scale
            })

        # å¯¹å½“å‰å°ºåº¦çš„åŒ¹é…è¿›è¡Œå±€éƒ¨NMS
        temp_matches.sort(key=lambda x: x['score'], reverse=True)
        filtered_matches = []
        for match in temp_matches:
            pt = match['pt']
            w, h = match['size']
            center_x = pt[0] + w // 2
            center_y = pt[1] + h // 2

            # æ›´ä¸¥æ ¼çš„é‡å æ£€æŸ¥
            overlap = False
            for selected in filtered_matches:
                s_pt = selected['pt']
                s_w, s_h = selected['size']
                s_center_x = s_pt[0] + s_w // 2
                s_center_y = s_pt[1] + s_h // 2

                # æ£€æŸ¥ä¸­å¿ƒç‚¹è·ç¦»å’ŒåŒºåŸŸé‡å 
                if (abs(center_x - s_center_x) < max(w, s_w) // 2 and
                        abs(center_y - s_center_y) < max(h, s_h) // 2):
                    overlap = True
                    break

            if not overlap:
                filtered_matches.append(match)

        all_matches.extend(filtered_matches)

    # å…¨å±€éæå¤§å€¼æŠ‘åˆ¶
    all_matches.sort(key=lambda x: x['score'], reverse=True)
    final_locations = []

    for match in all_matches:
        pt = match['pt']
        w, h = match['size']
        center_x = pt[0] + w // 2 + xdia
        center_y = pt[1] + h // 2 + ydia

        # æ›´ä¸¥æ ¼çš„é‡å æ£€æŸ¥
        overlap = False
        for selected in final_locations:
            s_x, s_y, s_w, s_h = selected
            s_center_x = s_x + s_w // 2
            s_center_y = s_y + s_h // 2

            # ä½¿ç”¨æ›´å°çš„é‡å é˜ˆå€¼
            if (abs(center_x - s_center_x) < min(w, s_w) // 2 and
                    abs(center_y - s_center_y) < min(h, s_h) // 2):
                overlap = True
                break

        if not overlap:
            final_locations.append((center_x, center_y, w, h))
            cv2.circle(video, (center_x, center_y), 4, (0, 255, 255), -1)

    return [(x, y) for x, y, _, _ in final_locations]